## build train and test sets
# split the dataset
# ~70% training / ~30% validation
smpSize <- floor(0.70 * length(corp))
#  set the seed to make your partition reproductible
set.seed(123)
trainInd <- sample(seq_len(nrow(dtm)), size = smpSize)
train <- dtm[trainInd, ]
test <- dtm[-trainInd, ]
# uncomment if you need to export a dataset to lda-c
#export2lda_c(train, "Posts.xml.w_ts.csv.2012-02.lda-c")
#setup parallel backend to use 8 processors
cl<-makeCluster(8)
registerDoParallel(cl)
cat("topicCount\tperp\talpha\tbeta.mean\tbeta.sd\tbeta.se\ttime (sec)\n", sep="\t", append = T
, file = paste(readFrom, ".perplexity", sep="")) # to file
foreach(topicCount = 2:nrow(dtm) #max = 1 topic per document
, .packages='topicmodels' #include package
) %dopar% { #change to %do% for sequential execution
startRun <- Sys.time()
lda <- LDA(train, topicCount)
perp <- perplexity(lda, test)
# get model params
mdl.alpha <- lda@alpha
mdl.beta.mean <- mean(lda@beta)
mdl.beta.sd <- sd(lda@beta)
mdl.beta.se <- sd(lda@beta) / sqrt(ncol(lda@beta) * nrow(lda@beta))
cat(topicCount, perp, "\n", sep="\t") #to screen (no screen output is parallel mode)
cat(topicCount, perp, mdl.alpha, mdl.beta.mean, mdl.beta.sd, mdl.beta.se , difftime(Sys.time(), startRun, units = "secs")
, "\n", sep="\t", append = T
, file = paste(readFrom, ".perplexity", sep="")) # to file
}
## Do basic LDA fit testing: split the dataset into testing and training (70/30)
## Iterate over topic count (k) from 2 to document count
## and output perplexity for each k
## Install and load packages
library(tm)
library(topicmodels)
library(foreach)
library(doParallel)
## source("C:\Users\Jorge\MSc\LDA\LDA RUNS\LDA DATA\utils.R")
x<-"DEBUG"
source("utils.R")
x
args <- commandArgs(trailingOnly = TRUE)
readFrom <- args[1]
corp <- createCorp("postsreduc.xml.csv", 2014, 12) ## JL changed from 2010 9, and readFrom
## Build a Document-Term Matrix
dtm <- DocumentTermMatrix(corp, control = list(minWordLength = 2)) #keep words of lenght 2 or longer
cat("Before tf-idf: term count =", ncol(dtm), ", doc count =", nrow(dtm), "\n")
dtm <- removeFrequentWords(dtm) #removing based on median tf-idf value
cat("After tf-idf: term count =", ncol(dtm), ", doc count =", nrow(dtm), "\n")
dtm <- removeSparseTerms(dtm, 1 - (1.1/nrow(dtm)) )  #remove terms appearing only in 1 document
cat("After removing terms appearing only in 1 document: term count =", ncol(dtm), ", doc count =", nrow(dtm), "\n")
## build train and test sets
# split the dataset
# ~70% training / ~30% validation
smpSize <- floor(0.70 * length(corp))
#  set the seed to make your partition reproductible
set.seed(123)
trainInd <- sample(seq_len(nrow(dtm)), size = smpSize)
train <- dtm[trainInd, ]
test <- dtm[-trainInd, ]
# uncomment if you need to export a dataset to lda-c
#export2lda_c(train, "Posts.xml.w_ts.csv.2012-02.lda-c")
#setup parallel backend to use 8 processors
cl<-makeCluster(8)
registerDoParallel(cl)
cat("topicCount\tperp\talpha\tbeta.mean\tbeta.sd\tbeta.se\ttime (sec)\n", sep="\t", append = T
, file = paste(readFrom, ".perplexity", sep="")) # to file
foreach(topicCount = 2:nrow(dtm) #max = 1 topic per document
, .packages='topicmodels' #include package
) %dopar% { #change to %do% for sequential execution
startRun <- Sys.time()
lda <- LDA(train, topicCount)
perp <- perplexity(lda, test)
# get model params
mdl.alpha <- lda@alpha
mdl.beta.mean <- mean(lda@beta)
mdl.beta.sd <- sd(lda@beta)
mdl.beta.se <- sd(lda@beta) / sqrt(ncol(lda@beta) * nrow(lda@beta))
cat(topicCount, perp, "\n", sep="\t") #to screen (no screen output is parallel mode)
cat(topicCount, perp, mdl.alpha, mdl.beta.mean, mdl.beta.sd, mdl.beta.se , difftime(Sys.time(), startRun, units = "secs")
, "\n", sep="\t", append = T
, file = paste(readFrom, ".perplexity", sep="")) # to file
}
## Do basic LDA fit testing: split the dataset into testing and training (70/30)
## Iterate over topic count (k) from 2 to document count
## and output perplexity for each k
## Install and load packages
library(tm)
library(topicmodels)
library(foreach)
library(doParallel)
## source("C:\Users\Jorge\MSc\LDA\LDA RUNS\LDA DATA\utils.R")
x<-"DEBUG"
source("utils.R")
x
args <- commandArgs(trailingOnly = TRUE)
readFrom <- args[1]
corp <- createCorp("postsreduc.xml.csv", 2014, 12) ## JL changed from 2010 9, and readFrom
## Build a Document-Term Matrix
dtm <- DocumentTermMatrix(corp, control = list(minWordLength = 2)) #keep words of lenght 2 or longer
cat("Before tf-idf: term count =", ncol(dtm), ", doc count =", nrow(dtm), "\n")
dtm <- removeFrequentWords(dtm) #removing based on median tf-idf value
cat("After tf-idf: term count =", ncol(dtm), ", doc count =", nrow(dtm), "\n")
dtm <- removeSparseTerms(dtm, 1 - (1.1/nrow(dtm)) )  #remove terms appearing only in 1 document
cat("After removing terms appearing only in 1 document: term count =", ncol(dtm), ", doc count =", nrow(dtm), "\n")
## build train and test sets
# split the dataset
# ~70% training / ~30% validation
smpSize <- floor(0.70 * length(corp))
#  set the seed to make your partition reproductible
set.seed(123)
trainInd <- sample(seq_len(nrow(dtm)), size = smpSize)
train <- dtm[trainInd, ]
test <- dtm[-trainInd, ]
# uncomment if you need to export a dataset to lda-c
#export2lda_c(train, "Posts.xml.w_ts.csv.2012-02.lda-c")
#setup parallel backend to use 8 processors
cl<-makeCluster(8)
registerDoParallel(cl)
cat("topicCount\tperp\talpha\tbeta.mean\tbeta.sd\tbeta.se\ttime (sec)\n", sep="\t", append = T
, file = paste(readFrom, ".perplexity", sep="")) # to file
foreach(topicCount = 2:nrow(dtm) #max = 1 topic per document
, .packages='topicmodels' #include package
) %dopar% { #change to %do% for sequential execution
startRun <- Sys.time()
lda <- LDA(train, topicCount)
perp <- perplexity(lda, test)
# get model params
mdl.alpha <- lda@alpha
mdl.beta.mean <- mean(lda@beta)
mdl.beta.sd <- sd(lda@beta)
mdl.beta.se <- sd(lda@beta) / sqrt(ncol(lda@beta) * nrow(lda@beta))
cat(topicCount, perp, "\n", sep="\t") #to screen (no screen output is parallel mode)
cat(topicCount, perp, mdl.alpha, mdl.beta.mean, mdl.beta.sd, mdl.beta.se , difftime(Sys.time(), startRun, units = "secs")
, "\n", sep="\t", append = T
, file = paste(readFrom, ".perplexity", sep="")) # to file
}
## Do basic LDA fit testing: split the dataset into testing and training (70/30)
## Iterate over topic count (k) from 2 to document count
## and output perplexity for each k
## Install and load packages
library(tm)
library(topicmodels)
library(foreach)
library(doParallel)
## source("C:\Users\Jorge\MSc\LDA\LDA RUNS\LDA DATA\utils.R")
x<-"DEBUG"
source("utils.R")
x
args <- commandArgs(trailingOnly = TRUE)
readFrom <- args[1]
corp <- createCorp("postsreduc.xml.csv", 2014, 12) ## JL changed from 2010 9, and readFrom
## Build a Document-Term Matrix
dtm <- DocumentTermMatrix(corp, control = list(minWordLength = 2)) #keep words of lenght 2 or longer
cat("Before tf-idf: term count =", ncol(dtm), ", doc count =", nrow(dtm), "\n")
dtm <- removeFrequentWords(dtm) #removing based on median tf-idf value
cat("After tf-idf: term count =", ncol(dtm), ", doc count =", nrow(dtm), "\n")
dtm <- removeSparseTerms(dtm, 1 - (1.1/nrow(dtm)) )  #remove terms appearing only in 1 document
cat("After removing terms appearing only in 1 document: term count =", ncol(dtm), ", doc count =", nrow(dtm), "\n")
## build train and test sets
# split the dataset
# ~70% training / ~30% validation
smpSize <- floor(0.70 * length(corp))
#  set the seed to make your partition reproductible
set.seed(123)
trainInd <- sample(seq_len(nrow(dtm)), size = smpSize)
train <- dtm[trainInd, ]
test <- dtm[-trainInd, ]
# uncomment if you need to export a dataset to lda-c
#export2lda_c(train, "Posts.xml.w_ts.csv.2012-02.lda-c")
#setup parallel backend to use 8 processors
cl<-makeCluster(8)
registerDoParallel(cl)
cat("topicCount\tperp\talpha\tbeta.mean\tbeta.sd\tbeta.se\ttime (sec)\n", sep="\t", append = T
, file = paste(readFrom, ".perplexity", sep="")) # to file
foreach(topicCount = 2:nrow(dtm) #max = 1 topic per document
, .packages='topicmodels' #include package
) %dopar% { #change to %do% for sequential execution
startRun <- Sys.time()
lda <- LDA(train, topicCount)
perp <- perplexity(lda, test)
# get model params
mdl.alpha <- lda@alpha
mdl.beta.mean <- mean(lda@beta)
mdl.beta.sd <- sd(lda@beta)
mdl.beta.se <- sd(lda@beta) / sqrt(ncol(lda@beta) * nrow(lda@beta))
cat(topicCount, perp, "\n", sep="\t") #to screen (no screen output is parallel mode)
cat(topicCount, perp, mdl.alpha, mdl.beta.mean, mdl.beta.sd, mdl.beta.se , difftime(Sys.time(), startRun, units = "secs")
, "\n", sep="\t", append = T
, file = paste(readFrom, ".perplexity", sep="")) # to file
}
## Do basic LDA fit testing: split the dataset into testing and training (70/30)
## Iterate over topic count (k) from 2 to document count
## and output perplexity for each k
## Install and load packages
library(tm)
library(topicmodels)
library(foreach)
library(doParallel)
## source("C:\Users\Jorge\MSc\LDA\LDA RUNS\LDA DATA\utils.R")
x<-"DEBUG"
source("utils.R")
x
args <- commandArgs(trailingOnly = TRUE)
readFrom <- args[1]
corp <- createCorp("postsreduc.xml.csv", 2014, 12) ## JL changed from 2010 9, and readFrom
## Build a Document-Term Matrix
dtm <- DocumentTermMatrix(corp, control = list(minWordLength = 2)) #keep words of lenght 2 or longer
cat("Before tf-idf: term count =", ncol(dtm), ", doc count =", nrow(dtm), "\n")
dtm <- removeFrequentWords(dtm) #removing based on median tf-idf value
cat("After tf-idf: term count =", ncol(dtm), ", doc count =", nrow(dtm), "\n")
dtm <- removeSparseTerms(dtm, 1 - (1.1/nrow(dtm)) )  #remove terms appearing only in 1 document
cat("After removing terms appearing only in 1 document: term count =", ncol(dtm), ", doc count =", nrow(dtm), "\n")
## build train and test sets
# split the dataset
# ~70% training / ~30% validation
smpSize <- floor(0.70 * length(corp))
#  set the seed to make your partition reproductible
set.seed(123)
trainInd <- sample(seq_len(nrow(dtm)), size = smpSize)
train <- dtm[trainInd, ]
test <- dtm[-trainInd, ]
# uncomment if you need to export a dataset to lda-c
#export2lda_c(train, "Posts.xml.w_ts.csv.2012-02.lda-c")
#setup parallel backend to use 8 processors
cl<-makeCluster(8)
registerDoParallel(cl)
cat("topicCount\tperp\talpha\tbeta.mean\tbeta.sd\tbeta.se\ttime (sec)\n", sep="\t", append = T
, file = paste(readFrom, ".perplexity", sep="")) # to file
foreach(topicCount = 2:nrow(dtm) #max = 1 topic per document
, .packages='topicmodels' #include package
) %dopar% { #change to %do% for sequential execution
startRun <- Sys.time()
lda <- LDA(train, topicCount)
perp <- perplexity(lda, test)
# get model params
mdl.alpha <- lda@alpha
mdl.beta.mean <- mean(lda@beta)
mdl.beta.sd <- sd(lda@beta)
mdl.beta.se <- sd(lda@beta) / sqrt(ncol(lda@beta) * nrow(lda@beta))
cat(topicCount, perp, "\n", sep="\t") #to screen (no screen output is parallel mode)
cat(topicCount, perp, mdl.alpha, mdl.beta.mean, mdl.beta.sd, mdl.beta.se , difftime(Sys.time(), startRun, units = "secs")
, "\n", sep="\t", append = T
, file = paste(readFrom, ".perplexity", sep="")) # to file
}
## Do basic LDA fit testing: split the dataset into testing and training (70/30)
## Iterate over topic count (k) from 2 to document count
## and output perplexity for each k
## Install and load packages
library(tm)
library(topicmodels)
library(foreach)
library(doParallel)
## source("C:\Users\Jorge\MSc\LDA\LDA RUNS\LDA DATA\utils.R")
x<-"DEBUG"
source("utils.R")
x
args <- commandArgs(trailingOnly = TRUE)
readFrom <- args[1]
corp <- createCorp("postsreduc.xml.csv", 2014, 12) ## JL changed from 2010 9, and readFrom
## Build a Document-Term Matrix
dtm <- DocumentTermMatrix(corp, control = list(minWordLength = 2)) #keep words of lenght 2 or longer
cat("Before tf-idf: term count =", ncol(dtm), ", doc count =", nrow(dtm), "\n")
dtm <- removeFrequentWords(dtm) #removing based on median tf-idf value
cat("After tf-idf: term count =", ncol(dtm), ", doc count =", nrow(dtm), "\n")
dtm <- removeSparseTerms(dtm, 1 - (1.1/nrow(dtm)) )  #remove terms appearing only in 1 document
cat("After removing terms appearing only in 1 document: term count =", ncol(dtm), ", doc count =", nrow(dtm), "\n")
## build train and test sets
# split the dataset
# ~70% training / ~30% validation
smpSize <- floor(0.70 * length(corp))
#  set the seed to make your partition reproductible
set.seed(123)
trainInd <- sample(seq_len(nrow(dtm)), size = smpSize)
train <- dtm[trainInd, ]
test <- dtm[-trainInd, ]
# uncomment if you need to export a dataset to lda-c
#export2lda_c(train, "Posts.xml.w_ts.csv.2012-02.lda-c")
#setup parallel backend to use 8 processors
cl<-makeCluster(8)
registerDoParallel(cl)
cat("topicCount\tperp\talpha\tbeta.mean\tbeta.sd\tbeta.se\ttime (sec)\n", sep="\t", append = T
, file = paste(readFrom, ".perplexity", sep="")) # to file
foreach(topicCount = 2:nrow(dtm) #max = 1 topic per document
, .packages='topicmodels' #include package
) %dopar% { #change to %do% for sequential execution
startRun <- Sys.time()
lda <- LDA(train, topicCount)
perp <- perplexity(lda, test)
# get model params
mdl.alpha <- lda@alpha
mdl.beta.mean <- mean(lda@beta)
mdl.beta.sd <- sd(lda@beta)
mdl.beta.se <- sd(lda@beta) / sqrt(ncol(lda@beta) * nrow(lda@beta))
cat(topicCount, perp, "\n", sep="\t") #to screen (no screen output is parallel mode)
cat(topicCount, perp, mdl.alpha, mdl.beta.mean, mdl.beta.sd, mdl.beta.se , difftime(Sys.time(), startRun, units = "secs")
, "\n", sep="\t", append = T
, file = paste(readFrom, ".perplexity", sep="")) # to file
}
source('C:/Users/Jorge/MSc/LDA/LDA RUNS/LDA DATA/do_lda_analysis.R', echo=TRUE)
source("utils.R")
source("utils.R")
source("utils.R")
source("utils.R")
source("utils.R")
source("utils.R")
source("utils.R")
x
args <- commandArgs(trailingOnly = TRUE)
readFrom <- args[1]
corp <- createCorp("postsreduc.xml.csv", 2014, 12) ## JL changed from 2010 9, and readFrom
## Build a Document-Term Matrix
dtm <- DocumentTermMatrix(corp, control = list(minWordLength = 2)) #keep words of lenght 2 or longer
cat("Before tf-idf: term count =", ncol(dtm), ", doc count =", nrow(dtm), "\n")
dtm <- removeFrequentWords(dtm) #removing based on median tf-idf value
cat("After tf-idf: term count =", ncol(dtm), ", doc count =", nrow(dtm), "\n")
source("xxutils.R")
x
args <- commandArgs(trailingOnly = TRUE)
readFrom <- args[1]
corp <- createCorp("postsreduc.xml.csv", 2014, 12) ## JL changed from 2010 9, and readFrom
## Build a Document-Term Matrix
dtm <- DocumentTermMatrix(corp, control = list(minWordLength = 2)) #keep words of lenght 2 or longer
cat("Before tf-idf: term count =", ncol(dtm), ", doc count =", nrow(dtm), "\n")
dtm <- removeFrequentWords(dtm) #removing based on median tf-idf value
cat("After tf-idf: term count =", ncol(dtm), ", doc count =", nrow(dtm), "\n")
dtm <- removeSparseTerms(dtm, 1 - (1.1/nrow(dtm)) )  #remove terms appearing only in 1 document
cat("After removing terms appearing only in 1 document: term count =", ncol(dtm), ", doc count =", nrow(dtm), "\n")
## build train and test sets
# split the dataset
# ~70% training / ~30% validation
smpSize <- floor(0.70 * length(corp))
install.packages("swirl")
library swirl()
library(swirl)
swirl()
5+7
x <- 5 + 7
x
y <- x - 3
y
c9(1.1, 9, 3.14)
c(1.1, 9, 3.14)
z <- c (1.1, 9, 3.14)
?c
z
x <- (z, 555, z)
x
c(z, 555, z)
z * 2 + 100
my_sqrt <- sqrt(z - 1)
my_sqrt
my_div <- z/my_sqrt
my_div
c(1, 2, 3, 4) + c(0, 10)
c(1, 2, 3, 4) + c(0, 10, 100)
z * 2 + 1000
my_div
>tm
?tm
setwd("C:/Users/Jorge/OneDrive/MSc/R/R Course")
setwd("C:/Users/Jorge/OneDrive/MSc/R/R Course/Assignment 1")
?read.table
f<-read.table("001.csv")
pwv
pwd
wd
setwd("C:/Users/Jorge/OneDrive/MSc/R/R Course/Assignment 1/specdata")
f <- read.table("001.csv")
f
f <- read.table("001.csv",sep=",")
f
?lapply
?list.files
directory<-"C:\users\Jorge\OneDrive\MSc\R\R Course\Assignment 1\specdata\"
filenames <- list.files(directory, pattern="*.csv", full_names=TRUE)
directory<-"C:\users\Jorge\OneDrive\MSc\R\R Course\Assignment 1\specdata\"
directory<-"\users\Jorge\OneDrive\MSc\R\R Course\Assignment 1\specdata\"
firectory <- "\\\\\\\\\\\\\dddddddddddd"
directory <- "kokokokokokokokokokokokok"
directory <- "\a\b"
directory <- "\a\b\c\cc\c\c\c\c\c\"
directory <- "\a\b\c"
directory <- ".\sprecdata\"
dir specdata
dir
ls specdata
ls -l
ls()
dir()
pwd
cd
directory
dir()
filenames <- list.files(path = ".", pattern="*.csv", full_names=TRUE)
filenames <- list.files(pattern="*.csv", full_names=TRUE)
list.files(pattern="*.csv")
?read.csv
?median
vec<-c(10, 5, 5)
median(vec)
filenames <- list.files(directory, pattern="*.csv", full_names=TRUE)
ldf <- lapply(filenames, read.csv)
filenames <- list.files(directory, pattern="*.csv", full_names=TRUE)
ldf <- lapply(filenames, read.csv)
filenames <- list.files(directory, pattern="*.csv")
ldf <- lapply(filenames, read.csv)
ldf
ldf(1)
ldf[1]
ldf[2]
ldf[[1]]
f <- read.csv("001.csv")
f
f[1]
f[2]
f[3]
mean(f)
?mean
mean(f, na.rm=TRUE)
?mean
f
mean(f)
mean("f")
mean("001.csv")
?sprintf
pollutantmean <- function(directory, pollutant, id = 1:10) {
filenames <- sprintf("%03d.csv", id)
filenames <- paste(directory, filenames, sep="/")
ldf <- lapply(filenames, read.csv)
df=ldply(ldf)
# df is your list of data.frames
mean(df[, pollutant], na.rm = TRUE)
}
pollutantmean("specdata", "sulfate", 1)
pollutantmean <- function(directory, pollutant, id = 1:10) {
filenames <- sprintf("%03d.csv", id)
filenames <- paste(directory, filenames, sep="\")
ldf <- lapply(filenames, read.csv)
df=ldply(ldf)
# df is your list of data.frames
mean(df[, pollutant], na.rm = TRUE)
}
}
[[[]]]
()
)))
""
::
:
A+B()
[-KPK[PK[PK[PK[PK[PKP]]]]]]
P[EDKFEKWEFKP[EFKOPEFK[OWEFKP[]]]]
OQEFK[P]
EFV
filenames <- list.files(directory, pattern="*.csv")
pollutantmean <- function(directory, pollutant, id = 1:10) {
filenames <- sprintf("%03d.csv", id)
filenames <- paste(directory, filenames, sep="\")
ldf <- lapply(filenames, read.csv)
df=ldply(ldf)
# df is your list of data.frames
mean(df[, pollutant], na.rm = TRUE)
}
filenames <- list.files(directory, pattern="*.csv")
f <- read.csv("001.csv")
pollutantmean <- function(directory, pollutant, id = 1:10) {
filenames <- sprintf("%03d.csv", id)
filenames <- paste(directory, filenames, sep="/")
ldf <- lapply(filenames, read.csv)
df=ldply(ldf)
# df is your list of data.frames
mean(df[, pollutant], na.rm = TRUE)
}
pollutantmean <- function(directory, pollutant, id = 1:10) {
filenames <- sprintf("%03d.csv", id)
filenames <- paste(directory, filenames, sep="\")
ldf <- lapply(filenames, read.csv)
df=ldply(ldf)
# df is your list of data.frames
mean(df[, pollutant], na.rm = TRUE)
}
filenames <- list.files(directory, pattern="*.csv")
}
filenames
file.access()
pollutantmean <- function(directory, pollutant, id = 1:10) {
filenames <- sprintf("%03d.csv", id)
filenames <- paste(directory, filenames, sep="/")
ldf <- lapply(filenames, read.csv)
df=ldply(ldf)
# df is your list of data.frames
mean(df[, pollutant], na.rm = TRUE)
}
pollutantmean <- function(directory, pollutant, id = 1:10) {
filenames <- sprintf("%03d.csv", id)
filenames <- paste(directory, filenames, sep="/")
ldf <- lapply(filenames, read.csv)
df=ldply(ldf)
# df is your list of data.frames
mean(df[, pollutant], na.rm = TRUE)
}
filenames <- paste(directory, filenames, sep="/")
filenames <- paste(directory, filenames, sep="\")
/
?paste
?paste
filenames <- paste(directory, filenames, sep='\')
q
